## Week5 - Solving Atari using Deep Reinforcement Learning
Welcome to Week5 of this project, this week you will solve the more complex atari environments of Gymnasium using DDQNs and ideas from [this](./Atari%20Paper.pdf) paper.
</br>
DDQNs addresses the overstimation bias possibility introduced due to random initialization of Neural Network, you should:
- Start by reading Pg 293 - Pg 297 of [Grokking RL](../GrokkingRL.pdf), this introduces you to the idea of DDQNs
- Then read the given [paper](./Atari%20Paper.pdf)
- Your final project is to make an RL agent capable of playing any one of the seven environments mentioned in the paper (Different environment have different training time, out of these agent for pong environment takes least time to train)
- This time you can use a Convolutional Neural Network to estimate the state-action value function Q(s,a), but the training might be slower because some of those environments have really large action space and you don't really have a way to input both the state 's' (which is is 2D array) and action 'a' (which is an integer), therefore, you have to use policy gradient for such environments where you estimate π(s, *) instead, i.e. your neural network takes s as input and outputs a vector containing the π(s, a) values of all possible actions 'a' for the state 's'. The main challenge in this implementation is that now you can no longer use the older loss function, you have to use the loss function given below. For any step in the trajectory you generate in the environment, you will have to pass current state and the following return 'G' (which should be calculated using the following reward values, and obviously have to be calculated at the end of trajectory) as training dataset. Then each step will contributes to a loss of -G*log(π(s, a)), you can sum all such losses and perform training as usual. The neural network will now take only the state s as input, but you have to make sure that the last layer in you network is a softmax layer to ensure that the values in the output vector are in range [0,1].
- You have to proceed by making the DDQN class in model.py and the RL agent in insert_environment_name.ipynb and solve the environment. I am not providing the files or any recommendations this time assuming you have enough experience by Week4 Assignment (As the final project implementation is very closely related to this)

All the best for your assignment and note that this week i.e. final project is strictly compulsory for your certification. The training times for any of the seven environments will be in hours, so make sure not to do training on last moment. Your code is expected to provide graphs of average rewards. You should provide a .gif file depicting the model playing, the final trained model file (.pth file)